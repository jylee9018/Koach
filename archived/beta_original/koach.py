import os
import subprocess
import whisper
from openai import OpenAI
import shutil
from pydub import AudioSegment
import textgrid
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import numpy as np
import json
from sentence_transformers import SentenceTransformer
import faiss
import torch
import librosa  # ì–µì–‘/ê°•ì„¸ ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€
from scipy.signal import find_peaks
import matplotlib.pyplot as plt
from datetime import datetime
import platform

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("Koach")


class KnowledgeBase:
    """í•œêµ­ì–´ ë°œìŒ ì§€ì‹ ë² ì´ìŠ¤"""

    def __init__(
        self,
        knowledge_dir: str = "knowledge",
        embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
    ):
        """
        Args:
            knowledge_dir: ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.knowledge_dir = knowledge_dir
        os.makedirs(knowledge_dir, exist_ok=True)

        # ë¬¸ì„œ ì €ì¥ì†Œ
        self.documents = []
        self.document_ids = []

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(embedding_model)

        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.index = None

        # ê¸°ë³¸ ì§€ì‹ ë¡œë“œ
        self.load_knowledge()

    def load_knowledge(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ë¡œë“œ"""
        logger.info("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")

        # ì§€ì‹ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSON íŒŒì¼ ë¡œë“œ
        file_paths = list(Path(self.knowledge_dir).glob("*.json"))

        if not file_paths:
            logger.warning("ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€ì‹ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            self.create_default_knowledge()
            file_paths = list(Path(self.knowledge_dir).glob("*.json"))

        # ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
        documents = []
        document_ids = []

        for file_path in file_paths:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                for item in data:
                    if "content" in item and "id" in item:
                        documents.append(item["content"])
                        document_ids.append(item["id"])
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")

        # ë¬¸ì„œ ì €ì¥
        self.documents = documents
        self.document_ids = document_ids

        # ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ êµ¬ì¶•
        self.build_index()

        logger.info(f"âœ… {len(self.documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

    def create_default_knowledge(self):
        """ê¸°ë³¸ ì§€ì‹ ìƒì„±"""
        logger.info("ê¸°ë³¸ í•œêµ­ì–´ ë°œìŒ ì§€ì‹ ìƒì„± ì¤‘...")

        # í•œêµ­ì–´ ë°œìŒ ê¸°ë³¸ ì§€ì‹
        basic_knowledge = [
            {
                "id": "consonants_basic",
                "content": "í•œêµ­ì–´ ììŒ(ì´ˆì„±): ã„±(g/k), ã„´(n), ã„·(d/t), ã„¹(r/l), ã…(m), ã…‚(b/p), ã……(s), ã…‡(silent/ng), ã…ˆ(j), ã…Š(ch), ã…‹(k), ã…Œ(t), ã…(p), ã…(h). ê° ììŒì€ ìœ„ì¹˜ì™€ ë°œì„± ë°©ë²•ì— ë”°ë¼ êµ¬ë¶„ëœë‹¤.",
            },
            {
                "id": "vowels_basic",
                "content": "í•œêµ­ì–´ ëª¨ìŒ: ã…(a), ã…“(eo), ã…—(o), ã…œ(u), ã…¡(eu), ã…£(i), ã…(ae), ã…”(e), ã…š(oe), ã…Ÿ(wi). ì… ëª¨ì–‘ê³¼ í˜€ì˜ ìœ„ì¹˜ê°€ ë°œìŒì— ì¤‘ìš”í•˜ë‹¤.",
            },
            {
                "id": "final_consonants",
                "content": "í•œêµ­ì–´ ë°›ì¹¨(ì¢…ì„±): ë°›ì¹¨ì€ ë‹¨ì–´ ëì— ì˜¤ëŠ” ììŒìœ¼ë¡œ, ë°œìŒì´ ì•½í™”ë˜ê±°ë‚˜ ë³€í˜•ë  ìˆ˜ ìˆë‹¤. ì£¼ìš” ë°›ì¹¨ ë°œìŒ: ã„±, ã„´, ã„·, ã„¹, ã…, ã…‚, ã…‡. íŠ¹íˆ ã„±, ã„·, ã…‚ì€ ë‹¨ì–´ ëì—ì„œ ë¶ˆíŒŒìŒìœ¼ë¡œ ë°œìŒí•œë‹¤.",
            },
            {
                "id": "pronunciation_rules",
                "content": "í•œêµ­ì–´ ë°œìŒ ê·œì¹™: 1) ì—°ìŒ í˜„ìƒ: ë°›ì¹¨ì´ ë‹¤ìŒ ìŒì ˆì˜ ì²« ì†Œë¦¬ë¡œ ë„˜ì–´ê°€ëŠ” í˜„ìƒ 2) ììŒ ë™í™”: ì¸ì ‘í•œ ììŒë¼ë¦¬ ì„œë¡œ ì˜í–¥ì„ ì£¼ì–´ ë¹„ìŠ·í•œ ì†Œë¦¬ë¡œ ë³€í•˜ëŠ” í˜„ìƒ 3) êµ¬ê°œìŒí™”: ã„·, ã…Œì´ ã…£ ëª¨ìŒ ì•ì—ì„œ ã…ˆ, ã…Šìœ¼ë¡œ ë³€í•˜ëŠ” í˜„ìƒ 4) ê²½ìŒí™”: íŠ¹ì • í™˜ê²½ì—ì„œ í‰ìŒì´ ê²½ìŒìœ¼ë¡œ ë³€í•˜ëŠ” í˜„ìƒ",
            },
            {
                "id": "common_foreigner_errors",
                "content": "ì™¸êµ­ì¸ í•™ìŠµìë“¤ì˜ í”í•œ ë°œìŒ ì‹¤ìˆ˜: 1) ã…“ì™€ ã…— êµ¬ë³„ ì–´ë ¤ì›€ 2) ã„¹ ë°œìŒ (ì˜ì–´ L/Rê³¼ ë‹¤ë¦„) 3) ë°›ì¹¨ ë°œìŒ ëˆ„ë½ 4) ê²½ìŒ(ã„²,ã„¸,ã…ƒ,ã…†,ã…‰)ê³¼ í‰ìŒ êµ¬ë³„ ì–´ë ¤ì›€ 5) ì¥ë‹¨ìŒ êµ¬ë³„ ë¶€ì¡± 6) ì—°ìŒ ê·œì¹™ ì ìš© ì‹¤íŒ¨",
            },
            {
                "id": "rhythm_intonation",
                "content": "í•œêµ­ì–´ ë¦¬ë“¬ê³¼ ì–µì–‘: í•œêµ­ì–´ëŠ” ìŒì ˆ ë‹¨ìœ„ì˜ ë¦¬ë“¬ì„ ê°€ì§„ë‹¤. ë¬¸ì¥ì˜ ì˜ë¯¸ì— ë”°ë¼ ì–µì–‘ íŒ¨í„´ì´ ë‹¬ë¼ì§„ë‹¤. ì˜ë¬¸ë¬¸ì€ ëì´ ì˜¬ë¼ê°€ê³ , í‰ì„œë¬¸ì€ ë‚´ë ¤ê°„ë‹¤. ê°ì • í‘œí˜„ì— ë”°ë¼ ì–µì–‘ì˜ êµ´ê³¡ì´ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.",
            },
            {
                "id": "practice_techniques",
                "content": "ë°œìŒ ì—°ìŠµ ê¸°ë²•: 1) ì… ëª¨ì–‘ ê±°ìš¸ë¡œ í™•ì¸í•˜ë©° ì—°ìŠµ 2) ë…¹ìŒí•˜ê³  ì›ì–´ë¯¼ê³¼ ë¹„êµ 3) ëŠë¦¬ê²Œ ë°œìŒí•œ í›„ ì ì°¨ ì†ë„ ë†’ì´ê¸° 4) ìµœì†ŒëŒ€ë¦½ìŒ(ë¹„ìŠ·í•œ ì†Œë¦¬ ë‹¨ì–´) ì—°ìŠµ 5) í˜€ ìœ„ì¹˜ ì˜ì‹í•˜ë©° ë°œìŒí•˜ê¸° 6) ê³¼ì¥ëœ ë°œìŒìœ¼ë¡œ ì‹œì‘í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì ˆ",
            },
            {
                "id": "tense_consonants",
                "content": "ê²½ìŒ(ëœì†Œë¦¬) ë°œìŒ: ã„², ã„¸, ã…ƒ, ã…†, ã…‰ëŠ” ì„±ëŒ€ë¥¼ ê¸´ì¥ì‹œí‚¤ê³  ê°•í•˜ê²Œ ë°œìŒí•œë‹¤. í‰ìŒê³¼ ë¹„êµ: 'ê°€ë‹¤(gada)'ì™€ 'ê¹Œë‹¤(kkada)', 'ë°”ë‹¤(bada)'ì™€ 'ë¹ ë‹¤(ppada)'ì˜ ì°¨ì´ ì¸ì‹í•˜ê¸°.",
            },
            {
                "id": "aspirated_consonants",
                "content": "ê²©ìŒ(ê±°ì„¼ì†Œë¦¬) ë°œìŒ: ã…‹, ã…Œ, ã…, ã…ŠëŠ” ê°•í•œ ê³µê¸°ë¥¼ ë‚´ë³´ë‚´ë©° ë°œìŒí•œë‹¤. 'ì¹´ë©”ë¼(kamera)', 'íƒ€ë‹¤(tada)', 'íŒŒë‹¤(pada)', 'ì°¨ë‹¤(chada)'ì—ì„œ ê±°ì„¼ ì†Œë¦¬ë¥¼ ëŠë‚„ ìˆ˜ ìˆë‹¤.",
            },
            {
                "id": "linking_sounds",
                "content": "ì—°ìŒ í˜„ìƒ: ë°›ì¹¨ì´ ìˆëŠ” ë‹¨ì–´ ë’¤ì— ëª¨ìŒìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¡°ì‚¬ë‚˜ ì–´ë¯¸ê°€ ì˜¤ë©´, ë°›ì¹¨ì€ ë‹¤ìŒ ìŒì ˆì˜ ì´ˆì„±ì²˜ëŸ¼ ë°œìŒëœë‹¤. ì˜ˆ: 'ê½ƒì´(ê¼¬ì¹˜)', 'ë°¥ì„(ë°”ë¸”)', 'ì±…ì€(ì±„ê·¼)'",
            },
        ]

        # ì§€ì‹ ì €ì¥
        with open(
            os.path.join(self.knowledge_dir, "basic_pronunciation.json"),
            "w",
            encoding="utf-8",
        ) as f:
            json.dump(basic_knowledge, f, ensure_ascii=False, indent=2)

        logger.info("âœ… ê¸°ë³¸ ì§€ì‹ ìƒì„± ì™„ë£Œ")

    def build_index(self):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not self.documents:
            logger.warning("ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(self.documents)

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(embeddings).astype("float32"))

        logger.info(f"âœ… {len(self.documents)}ê°œ ë¬¸ì„œì— ëŒ€í•œ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """ì§ˆì˜ì— ê´€ë ¨ëœ ì§€ì‹ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

        Returns:
            List[Dict]: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.index or not self.documents:
            logger.warning("ê²€ìƒ‰í•  ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ì§ˆì˜ ì„ë² ë”©
        query_embedding = self.model.encode([query])

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        scores, indices = self.index.search(
            np.array(query_embedding).astype("float32"),
            k=min(top_k, len(self.documents)),
        )

        # ê²°ê³¼ ì •ë¦¬
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents) and idx >= 0:
                results.append(
                    {
                        "id": self.document_ids[idx],
                        "content": self.documents[idx],
                        "score": float(scores[0][i]),
                    }
                )

        return results

    def add_document(self, doc_id: str, content: str):
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€

        Args:
            doc_id: ë¬¸ì„œ ID
            content: ë¬¸ì„œ ë‚´ìš©
        """
        # ë¬¸ì„œ ì¶”ê°€
        self.documents.append(content)
        self.document_ids.append(doc_id)

        # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        self.build_index()


class ProsodyAnalyzer:
    """ì–µì–‘ê³¼ ê°•ì„¸ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self):
        self.pitch_threshold = 0.5
        self.energy_threshold = 0.6

    def analyze_pitch(self, audio_path: str) -> Dict:
        """í”¼ì¹˜(ìŒë†’ì´) ë¶„ì„"""
        try:
            y, sr = librosa.load(audio_path)
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)

            pitch_mean = np.mean(pitches)
            pitch_std = np.std(pitches)
            pitch_range = np.max(pitches) - np.min(pitches)

            return {
                "mean": pitch_mean,
                "std": pitch_std,
                "range": pitch_range,
                "raw_pitches": pitches,
            }
        except Exception as e:
            logger.error(f"í”¼ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def analyze_energy(self, audio_path: str) -> Dict:
        """ì—ë„ˆì§€(ê°•ì„¸) ë¶„ì„"""
        try:
            y, sr = librosa.load(audio_path)
            energy = librosa.feature.rms(y=y)[0]

            energy_mean = np.mean(energy)
            energy_std = np.std(energy)

            # í”¼í¬ ê²€ì¶œ ìˆ˜ì •
            # scipy.signal.find_peaks ì‚¬ìš©
            peaks, _ = find_peaks(energy, height=np.mean(energy), distance=3)

            return {
                "mean": energy_mean,
                "std": energy_std,
                "peaks": peaks,
                "raw_energy": energy,
            }
        except Exception as e:
            logger.error(f"ì—ë„ˆì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def analyze_time_domain(self, audio_path: str) -> Dict:
        """ì‹œê°„ ì˜ì—­ ë¶„ì„"""
        try:
            y, sr = librosa.load(audio_path)

            # ìŒì ˆ ë‹¨ìœ„ ë¶„ì„
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)
            tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)

            # ë°œí™” ì†ë„ ê³„ì‚°
            duration = librosa.get_duration(y=y, sr=sr)

            return {"tempo": tempo, "duration": duration, "onset_strength": onset_env}
        except Exception as e:
            logger.error(f"ì‹œê°„ ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def analyze_sentence_structure(self, text: str, timing: str) -> Dict:
        """ë¬¸ì¥ êµ¬ì¡° ê¸°ë°˜ ë¶„ì„"""
        try:
            # ë¬¸ì¥ ë ë¶€ë¶„ì˜ ì–µì–‘ íŒ¨í„´ ë¶„ì„
            sentence_end = timing.split("\n")[-10:]  # ë§ˆì§€ë§‰ 10ê°œ ìŒì†Œ

            # ì˜ë¬¸ë¬¸/í‰ì„œë¬¸ êµ¬ë¶„
            is_question = text.strip().endswith("?")

            return {
                "sentence_type": "question" if is_question else "statement",
                "end_pattern": sentence_end,
            }
        except Exception as e:
            logger.error(f"ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def adjust_thresholds(self, native_audio: str):
        """ì›ì–´ë¯¼ ë°œí™” ê¸°ë°˜ ì„ê³„ê°’ ì¡°ì •"""
        try:
            native_energy = self.analyze_energy(native_audio)
            if native_energy:
                self.energy_threshold = native_energy["mean"] * 0.8

            native_pitch = self.analyze_pitch(native_audio)
            if native_pitch:
                self.pitch_threshold = native_pitch["mean"] * 0.8
        except Exception as e:
            logger.error(f"ì„ê³„ê°’ ì¡°ì • ì‹¤íŒ¨: {e}")

    def compare_prosody(
        self,
        learner_audio: str,
        native_audio: str,
        learner_text: str = None,
        learner_timing: str = None,
    ) -> Dict:
        """í•™ìŠµìì™€ ì›ì–´ë¯¼ì˜ ì–µì–‘/ê°•ì„¸ ë¹„êµ"""
        # ê¸°ë³¸ ë¶„ì„
        learner_pitch = self.analyze_pitch(learner_audio)
        native_pitch = self.analyze_pitch(native_audio)
        learner_energy = self.analyze_energy(learner_audio)
        native_energy = self.analyze_energy(native_audio)

        # ì‹œê°„ ì˜ì—­ ë¶„ì„ ì¶”ê°€
        learner_time = self.analyze_time_domain(learner_audio)
        native_time = self.analyze_time_domain(native_audio)

        # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ (í…ìŠ¤íŠ¸ì™€ íƒ€ì´ë° ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        learner_structure = None
        if learner_text and learner_timing:
            learner_structure = self.analyze_sentence_structure(
                learner_text, learner_timing
            )

        if not all(
            [
                learner_pitch,
                native_pitch,
                learner_energy,
                native_energy,
                learner_time,
                native_time,
            ]
        ):
            return None

        # ì„ê³„ê°’ ë™ì  ì¡°ì •
        self.adjust_thresholds(native_audio)

        # í”¼ì¹˜ ë¹„êµ
        pitch_diff = {
            "mean_diff": learner_pitch["mean"] - native_pitch["mean"],
            "std_diff": learner_pitch["std"] - native_pitch["std"],
            "range_diff": learner_pitch["range"] - native_pitch["range"],
        }

        # ì—ë„ˆì§€ ë¹„êµ
        energy_diff = {
            "mean_diff": learner_energy["mean"] - native_energy["mean"],
            "std_diff": learner_energy["std"] - native_energy["std"],
            "peaks_diff": len(learner_energy["peaks"]) - len(native_energy["peaks"]),
        }

        # ì‹œê°„ ì˜ì—­ ë¹„êµ
        time_diff = {
            "tempo_diff": learner_time["tempo"] - native_time["tempo"],
            "duration_ratio": learner_time["duration"] / native_time["duration"],
            "onset_strength_diff": np.mean(learner_time["onset_strength"])
            - np.mean(native_time["onset_strength"]),
        }

        result = {"pitch": pitch_diff, "energy": energy_diff, "time": time_diff}

        if learner_structure:
            result["structure"] = learner_structure

        return result

    def visualize_prosody(
        self,
        learner_audio: str,
        native_audio: str,
        output_path: str = None,
        learner_textgrid: str = None,
        native_textgrid: str = None,
    ) -> None:
        """ì–µì–‘ê³¼ ê°•ì„¸ ì‹œê°í™”

        Args:
            learner_audio: í•™ìŠµì ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            native_audio: ì›ì–´ë¯¼ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í™”ë©´ì— í‘œì‹œ)
            learner_textgrid: í•™ìŠµì TextGrid íŒŒì¼ ê²½ë¡œ
            native_textgrid: ì›ì–´ë¯¼ TextGrid íŒŒì¼ ê²½ë¡œ
        """
        try:
            # platform ëª¨ë“ˆ import ì¶”ê°€
            import platform
            import matplotlib.font_manager as fm

            # í•œê¸€ í°íŠ¸ ì„¤ì •
            if platform.system() == "Darwin":  # macOS
                plt.rcParams["font.family"] = "AppleGothic"
            elif platform.system() == "Windows":
                plt.rcParams["font.family"] = "Malgun Gothic"
            else:  # Linux
                plt.rcParams["font.family"] = "NanumGothic"
            plt.rcParams["axes.unicode_minus"] = False

            # ì˜¤ë””ì˜¤ ë¡œë“œ
            learner_y, sr = librosa.load(learner_audio)
            native_y, sr = librosa.load(native_audio)

            # í”¼ì¹˜ ì¶”ì¶œ
            learner_pitches, learner_magnitudes = librosa.piptrack(y=learner_y, sr=sr)
            native_pitches, native_magnitudes = librosa.piptrack(y=native_y, sr=sr)

            # ì—ë„ˆì§€ ì¶”ì¶œ
            learner_energy = librosa.feature.rms(y=learner_y)[0]
            native_energy = librosa.feature.rms(y=native_y)[0]

            # ì‹œê°„ ì¶• ìƒì„±
            learner_times = np.arange(len(learner_energy)) * (
                len(learner_y) / sr / len(learner_energy)
            )
            native_times = np.arange(len(native_energy)) * (
                len(native_y) / sr / len(native_energy)
            )

            # TextGridì—ì„œ ìŒì†Œ ì •ë³´ ì¶”ì¶œ
            def extract_phonemes(textgrid_path):
                if not textgrid_path:
                    return []
                tg = textgrid.TextGrid.fromFile(textgrid_path)
                phonemes = []
                for tier in tg.tiers:
                    if tier.name.lower() in ["phones", "phoneme", "phone"]:
                        for interval in tier:
                            if interval.mark.strip():
                                phonemes.append(
                                    {
                                        "text": interval.mark,
                                        "start": interval.minTime,
                                        "end": interval.maxTime,
                                    }
                                )
                return phonemes

            learner_phonemes = extract_phonemes(learner_textgrid)
            native_phonemes = extract_phonemes(native_textgrid)

            # ê·¸ë˜í”„ ìƒì„±
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

            # í”¼ì¹˜ ê·¸ë˜í”„
            ax1.plot(
                learner_times,
                np.mean(learner_pitches, axis=0),
                label="í•™ìŠµì",
                color="red",
                alpha=0.7,
            )
            ax1.plot(
                native_times,
                np.mean(native_pitches, axis=0),
                label="ì›ì–´ë¯¼",
                color="blue",
                alpha=0.7,
            )
            ax1.set_title("ìŒë†’ì´(í”¼ì¹˜) ë¹„êµ")
            ax1.set_xlabel("ì‹œê°„ (ì´ˆ)")
            ax1.set_ylabel("í”¼ì¹˜ (Hz)")
            ax1.legend()
            ax1.grid(True)

            # ì—ë„ˆì§€ ê·¸ë˜í”„
            ax2.plot(
                learner_times, learner_energy, label="í•™ìŠµì", color="red", alpha=0.7
            )
            ax2.plot(
                native_times, native_energy, label="ì›ì–´ë¯¼", color="blue", alpha=0.7
            )
            ax2.set_title("ê°•ì„¸(ì—ë„ˆì§€) ë¹„êµ")
            ax2.set_xlabel("ì‹œê°„ (ì´ˆ)")
            ax2.set_ylabel("ì—ë„ˆì§€")
            ax2.legend()
            ax2.grid(True)

            # ìŒì†Œ ì •ë³´ í‘œì‹œ
            def add_phoneme_annotations(ax, phonemes, y_pos):
                for phoneme in phonemes:
                    ax.annotate(
                        phoneme["text"],
                        xy=((phoneme["start"] + phoneme["end"]) / 2, y_pos),
                        xytext=((phoneme["start"] + phoneme["end"]) / 2, y_pos),
                        ha="center",
                        va="bottom",
                        rotation=45,
                        fontsize=8,
                    )

            # ê° ê·¸ë˜í”„ì— ìŒì†Œ ì •ë³´ ì¶”ê°€
            add_phoneme_annotations(ax1, learner_phonemes, ax1.get_ylim()[0])
            add_phoneme_annotations(ax2, learner_phonemes, ax2.get_ylim()[0])

            plt.tight_layout()

            # ì €ì¥ ë˜ëŠ” í‘œì‹œ
            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches="tight")
                plt.close()
            else:
                plt.show()

        except Exception as e:
            logger.error(f"ì‹œê°í™” ì‹¤íŒ¨: {e}")


class Koach:
    """í•œêµ­ì–´ ë°œìŒ í‰ê°€ ë° í”¼ë“œë°± ì‹œìŠ¤í…œ"""

    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: ì„¤ì • íŒŒë¼ë¯¸í„° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        # ê¸°ë³¸ ì„¤ì •
        self.config = {
            # íŒŒì¼ ê²½ë¡œ
            "learner_audio": "input/learner.m4a",
            "native_audio": "input/native.m4a",
            "output_dir": "output",
            "wav_dir": "wav",
            "mfa_input_dir": "mfa_input",
            "mfa_output_dir": "aligned",
            # ëª¨ë¸ ê²½ë¡œ
            "lexicon_path": "models/korean_mfa.dict",
            "acoustic_model": "models/korean_mfa.zip",
            # Whisper ëª¨ë¸ í¬ê¸°
            "whisper_model": "base",
            # OpenAI ëª¨ë¸
            "openai_model": "gpt-4o",
            # RAG ì„¤ì •
            "use_rag": True,
            "knowledge_dir": "knowledge",
            "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
        }

        # ì‚¬ìš©ì ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if config:
            self.config.update(config)

        # OpenAI API í‚¤ ì„¤ì •
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            logger.warning("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self._setup_paths()

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()

        # RAG ì§€ì‹ ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì„¤ì •ì— ë”°ë¼)
        if self.config["use_rag"]:
            self.knowledge_base = KnowledgeBase(
                knowledge_dir=self.config["knowledge_dir"],
                embedding_model=self.config["embedding_model"],
            )
        else:
            self.knowledge_base = None

        # ì–µì–‘/ê°•ì„¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.prosody_analyzer = ProsodyAnalyzer()

    # [ê¸°ì¡´ ë©”ì†Œë“œë“¤ì€ ë™ì¼í•˜ê²Œ ìœ ì§€]

    def _setup_paths(self):
        """íŒŒì¼ ê²½ë¡œ ì„¤ì •"""
        # ì…ë ¥ íŒŒì¼
        self.learner_audio = self.config["learner_audio"]
        self.native_audio = self.config["native_audio"]

        # ë””ë ‰í† ë¦¬
        self.wav_dir = self.config["wav_dir"]
        self.mfa_input = self.config["mfa_input_dir"]
        self.mfa_output = self.config["mfa_output_dir"]

        # ë³€í™˜ëœ WAV íŒŒì¼
        self.learner_wav = os.path.join(self.wav_dir, "learner.wav")
        self.native_wav = os.path.join(self.wav_dir, "native.wav")

        # Whisper ì „ì‚¬ ê²°ê³¼
        self.learner_transcript = os.path.join(self.wav_dir, "learner.txt")
        self.native_transcript = os.path.join(self.wav_dir, "native.txt")

        # ì •ë‹µ ìŠ¤í¬ë¦½íŠ¸
        self.script_path = os.path.join(self.wav_dir, "script.txt")

        # MFA ê´€ë ¨ íŒŒì¼
        self.lexicon_path = self.config["lexicon_path"]
        self.acoustic_model = self.config["acoustic_model"]

        # TextGrid íŒŒì¼
        self.learner_textgrid = os.path.join(self.mfa_output, "learner.TextGrid")
        self.native_textgrid = os.path.join(self.mfa_output, "native.TextGrid")

    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.wav_dir, exist_ok=True)
        os.makedirs(self.mfa_input, exist_ok=True)
        os.makedirs(self.mfa_output, exist_ok=True)
        if self.config["use_rag"]:
            os.makedirs(self.config["knowledge_dir"], exist_ok=True)

    def convert_audio(self, input_path: str, output_path: str) -> bool:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            input_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ WAV íŒŒì¼ ê²½ë¡œ

        Returns:
            bool: ë³€í™˜ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸ§ ë³€í™˜ ì¤‘: {input_path} â†’ {output_path}")
            audio = AudioSegment.from_file(input_path)
            audio = audio.set_channels(1).set_frame_rate(16000)
            audio.export(output_path, format="wav")
            logger.info("âœ… ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    def transcribe_audio(self, wav_path: str, transcript_path: str) -> Optional[str]:
        """Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬

        Args:
            wav_path: WAV íŒŒì¼ ê²½ë¡œ
            transcript_path: ì „ì‚¬ ê²°ê³¼ ì €ì¥ ê²½ë¡œ

        Returns:
            Optional[str]: ì „ì‚¬ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            logger.info(f"ğŸ“ Whisper ì „ì‚¬ ì¤‘: {wav_path}")
            model = whisper.load_model(self.config["whisper_model"])
            result = model.transcribe(wav_path, language="ko")

            with open(transcript_path, "w", encoding="utf-8") as f:
                f.write(result["text"])

            logger.info(f"ğŸ“„ ì „ì‚¬ ê²°ê³¼: {result['text']}")
            return result["text"]
        except Exception as e:
            logger.error(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return None

    def run_mfa_alignment(
        self, wav_path: str, transcript_path: str, output_name: str
    ) -> bool:
        """MFAë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ ì •ë ¬

        Args:
            wav_path: WAV íŒŒì¼ ê²½ë¡œ
            transcript_path: ì „ì‚¬ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œì™¸)

        Returns:
            bool: ì •ë ¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸ”§ MFA ì •ë ¬ ì‹œì‘: {output_name}")

            # íŒŒì¼ ë³µì‚¬
            shutil.copy(wav_path, os.path.join(self.mfa_input, f"{output_name}.wav"))
            shutil.copy(
                transcript_path, os.path.join(self.mfa_input, f"{output_name}.txt")
            )

            # MFA ì •ë ¬ ì‹¤í–‰
            command = [
                "mfa",
                "align",
                self.mfa_input,
                self.lexicon_path,
                self.acoustic_model,
                self.mfa_output,
                "--clean",
                "--no_text_cleaning",
            ]

            result = subprocess.run(
                command, capture_output=True, text=True, check=False
            )

            if result.returncode != 0:
                logger.error(f"MFA ì •ë ¬ ì‹¤íŒ¨: {result.stderr}")
                return False

            logger.info("âœ… MFA ì •ë ¬ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"MFA ì •ë ¬ ì‹¤íŒ¨: {e}")
            return False

    def summarize_textgrid(self, path: str) -> Optional[str]:
        """TextGrid íŒŒì¼ì—ì„œ ìŒì†Œ ì •ë³´ ì¶”ì¶œ

        Args:
            path: TextGrid íŒŒì¼ ê²½ë¡œ

        Returns:
            Optional[str]: ìŒì†Œ ì •ë³´ ìš”ì•½ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            logger.info(f"ğŸ“Š TextGrid ìš”ì•½ ì¤‘: {path}")
            tg = textgrid.TextGrid.fromFile(path)
            summary = []

            for tier in tg.tiers:
                if tier.name.lower() in ["phones", "phoneme", "phone"]:
                    for interval in tier:
                        if interval.mark.strip():
                            summary.append(
                                f"{interval.mark}: {round(interval.minTime, 2)}s ~ {round(interval.maxTime, 2)}s"
                            )

            return "\n".join(summary)
        except Exception as e:
            logger.error(f"TextGrid ìš”ì•½ ì‹¤íŒ¨: {e}")
            return None

    def extract_pronunciation_issues(
        self, learner_text: str, native_text: str, learner_timing: str
    ) -> List[str]:
        """ë°œìŒ ë¬¸ì œì  ì¶”ì¶œ

        Args:
            learner_text: í•™ìŠµì í…ìŠ¤íŠ¸
            native_text: ì›ì–´ë¯¼ í…ìŠ¤íŠ¸
            learner_timing: í•™ìŠµì íƒ€ì´ë° ì •ë³´

        Returns:
            List[str]: ë°œê²¬ëœ ë¬¸ì œì  ëª©ë¡
        """
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë¬¸ì œ ì¶”ì¶œ
        issues = []

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ ì°¨ì´ (ë„ˆë¬´ í¬ë©´ ë°œìŒ ëˆ„ë½ ê°€ëŠ¥ì„±)
        if len(native_text) > len(learner_text) * 1.2:
            issues.append("ë°œìŒ ëˆ„ë½ ê°€ëŠ¥ì„± ìˆìŒ")

        # 2. ë°›ì¹¨ ê´€ë ¨ ë¬¸ì œ ê²€ì¶œ
        learner_words = learner_text.split()
        native_words = native_text.split()

        for n_word in native_words:
            if len(n_word) >= 2 and n_word not in learner_text:
                # ë°›ì¹¨ì´ ìˆëŠ” ë‹¨ì–´ë©´ ë°›ì¹¨ ê´€ë ¨ ë¬¸ì œ ê°€ëŠ¥ì„± ì¶”ê°€
                if any(c in "ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…" for c in n_word):
                    issues.append(f"'{n_word}' ë‹¨ì–´ ë°œìŒ ë¬¸ì œ (ë°›ì¹¨ ê´€ë ¨)")

        # 3. ê³µí†µ ìŒì†Œ ë¬¸ì œ (ìŒì†Œ ê¸¸ì´, ìŒìƒ‰ ë“±)
        phoneme_issues = set()
        for line in learner_timing.split("\n"):
            parts = line.split(":")
            if len(parts) >= 2:
                phoneme = parts[0].strip()

                # ëª¨ìŒ ê´€ë ¨ ë¬¸ì œ ê²€ì¶œ
                if phoneme in ["ã…“", "ã…—", "ã…œ", "ã…¡"]:
                    phoneme_issues.add(f"{phoneme} ëª¨ìŒ ë°œìŒ")

                # ê²½ìŒ/ê²©ìŒ ê´€ë ¨ ë¬¸ì œ ê²€ì¶œ
                if phoneme in ["ã„²", "ã„¸", "ã…ƒ", "ã…†", "ã…‰", "ã…‹", "ã…Œ", "ã…", "ã…Š"]:
                    phoneme_issues.add(f"{phoneme} ììŒ ë°œìŒ")

        issues.extend(list(phoneme_issues))

        return issues

    def generate_prompt(
        self,
        learner_text: str,
        native_text: str,
        script_text: str,
        learner_timing: str,
        native_timing: str,
        prosody_feedback: Optional[List[str]] = None,
    ) -> str:
        """GPT í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            learner_text: í•™ìŠµì ë°œí™” í…ìŠ¤íŠ¸
            native_text: ì›ì–´ë¯¼ ë°œí™” í…ìŠ¤íŠ¸
            script_text: ëª©í‘œ ìŠ¤í¬ë¦½íŠ¸
            learner_timing: í•™ìŠµì ìŒì†Œ ì •ë ¬ ì •ë³´
            native_timing: ì›ì–´ë¯¼ ìŒì†Œ ì •ë ¬ ì •ë³´
            prosody_feedback: ì–µì–‘/ê°•ì„¸ ë¶„ì„ ê²°ê³¼

        Returns:
            str: GPT í”„ë¡¬í”„íŠ¸
        """
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì–´ í•™ìŠµìì˜ ë°œí™” ì •ë³´ì™€ ì›ì–´ë¯¼ì˜ ì˜ˆì‹œ ë°œí™” ì •ë³´ì…ë‹ˆë‹¤.

# í•™ìŠµì ë°œí™” í…ìŠ¤íŠ¸:
"{learner_text}"

# ì›ì–´ë¯¼ ë°œí™” í…ìŠ¤íŠ¸:
"{native_text}"

# ëª©í‘œ ìŠ¤í¬ë¦½íŠ¸:
"{script_text}"

# í•™ìŠµìì˜ ìŒì†Œ ì •ë ¬ ì •ë³´:
{learner_timing}

# ì›ì–´ë¯¼ì˜ ìŒì†Œ ì •ë ¬ ì •ë³´:
{native_timing}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì¤˜:

1. í•™ìŠµìì˜ ë°œìŒì—ì„œ ëˆ„ë½ë˜ê±°ë‚˜ ë¶€ì •í™•í•œ ë‹¨ì–´ë‚˜ ìŒì†ŒëŠ” ë¬´ì—‡ì¸ê°€?
   - êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ

2. í•™ìŠµìì˜ ë°œìŒì—ì„œ ë¶€ì ì ˆí•˜ê²Œ ë„ì–´ ì½ì€ ë‹¨ì–´ë‚˜ ìŒì†ŒëŠ” ë¬´ì—‡ì¸ê°€?  
   - ê¼­ í•´ë‹¹í•˜ëŠ” **ë‹¨ì–´ë‚˜ ìŒì†Œ**ë¥¼ í•¨ê»˜ ì œì‹œ

3. ì›ì–´ë¯¼ê³¼ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ **ë‹¨ì–´ë‚˜ êµ¬ì ˆì—ì„œ** ì†ë„ ì°¨ì´ê°€ ìˆëŠ”ê°€?  
   - ì†ë„ ì •ë³´ë¥¼ ì œì‹œí•  ë•ŒëŠ” ê¼­ í•´ë‹¹í•˜ëŠ” **ë‹¨ì–´ë‚˜ ìŒì†Œ**ë¥¼ í•¨ê»˜ ì œì‹œ

4. ë” ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ë°œìŒí•˜ê¸° ìœ„í•œ íŒì„ ê°„ë‹¨íˆ ì œì‹œ
"""

        # RAGê°€ í™œì„±í™”ëœ ê²½ìš°, ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰ ë° ì¶”ê°€
        if self.config["use_rag"] and self.knowledge_base:
            # ë°œìŒ ë¬¸ì œì  ì¶”ì¶œ
            issues = self.extract_pronunciation_issues(
                learner_text, native_text, learner_timing
            )

            # ì¿¼ë¦¬ ìƒì„±
            query = f"í•œêµ­ì–´ ë°œìŒ: {' '.join(issues)}"

            # ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
            relevant_docs = self.knowledge_base.search(query, top_k=3)

            if relevant_docs:
                prompt += "\n\n# ì°¸ê³ í•  ë°œìŒ ì§€ì‹:\n"
                for doc in relevant_docs:
                    prompt += f"- {doc['content']}\n"

                prompt += "\nìœ„ ì°¸ê³  ì§€ì‹ì„ í™œìš©í•˜ì—¬ í•™ìŠµìì—ê²Œ ë” êµ¬ì²´ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”."

        # ì–µì–‘/ê°•ì„¸ í”¼ë“œë°± ì¶”ê°€
        if prosody_feedback:
            prompt += "\n\n# ì–µì–‘ê³¼ ê°•ì„¸ ë¶„ì„ ê²°ê³¼:\n"
            for feedback in prosody_feedback:
                prompt += f"- {feedback}\n"

            prompt += "\nìœ„ ì–µì–‘ê³¼ ê°•ì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ë” ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒì´ ë˜ë„ë¡ ì¡°ì–¸í•´ì£¼ì„¸ìš”."

        return prompt

    def get_feedback(self, prompt: str) -> Optional[str]:
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í”¼ë“œë°± ìƒì„±

        Args:
            prompt: GPT í”„ë¡¬í”„íŠ¸

        Returns:
            Optional[str]: ìƒì„±ëœ í”¼ë“œë°± (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            logger.info("ğŸ¤– GPT í”¼ë“œë°± ìƒì„± ì¤‘...")

            if not self.api_key:
                logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None

            client = OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.config["openai_model"],
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ ë°œìŒ ê°•ì‚¬ì…ë‹ˆë‹¤. í•™ìŠµìê°€ ì™¸êµ­ì¸ì„ì„ ê³ ë ¤í•˜ì—¬ ì‰¬ìš´ ë¬¸ë²• ìš©ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    },
                    {"role": "user", "content": prompt},
                ],
            )

            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def analyze_prosody(
        self,
        learner_audio: str,
        native_audio: str,
        learner_text: str = None,
        learner_timing: str = None,
        visualize: bool = True,
    ) -> Dict:
        """ì–µì–‘ê³¼ ê°•ì„¸ ë¶„ì„"""
        try:
            prosody_result = self.prosody_analyzer.compare_prosody(
                learner_audio, native_audio, learner_text, learner_timing
            )

            if not prosody_result:
                return {"error": "ì–µì–‘/ê°•ì„¸ ë¶„ì„ ì‹¤íŒ¨"}

            feedback = []

            # í”¼ì¹˜ ê´€ë ¨ í”¼ë“œë°±
            pitch = prosody_result["pitch"]
            if abs(pitch["mean_diff"]) > self.prosody_analyzer.pitch_threshold:
                feedback.append(
                    f"ì „ì²´ì ì¸ ìŒë†’ì´ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'ë†’' if pitch['mean_diff'] > 0 else 'ë‚®'}ìŠµë‹ˆë‹¤."
                )
            if abs(pitch["range_diff"]) > self.prosody_analyzer.pitch_threshold * 1.5:
                feedback.append(
                    f"ìŒë†’ì´ ë³€í™”ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'í¬' if pitch['range_diff'] > 0 else 'ì‘'}ìŠµë‹ˆë‹¤."
                )

            # ê°•ì„¸ ê´€ë ¨ í”¼ë“œë°±
            energy = prosody_result["energy"]
            if abs(energy["mean_diff"]) > self.prosody_analyzer.energy_threshold:
                feedback.append(
                    f"ì „ì²´ì ì¸ ê°•ì„¸ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'ê°•' if energy['mean_diff'] > 0 else 'ì•½'}í•©ë‹ˆë‹¤."
                )
            if abs(energy["peaks_diff"]) > 2:
                feedback.append(
                    f"ê°•ì„¸ì˜ ë³€í™”ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'ë§' if energy['peaks_diff'] > 0 else 'ì '}ìŠµë‹ˆë‹¤."
                )

            # ì‹œê°„ ì˜ì—­ í”¼ë“œë°±
            time = prosody_result["time"]
            if abs(time["tempo_diff"]) > 10:  # BPM ì°¨ì´ê°€ 10 ì´ìƒ
                feedback.append(
                    f"ë°œí™” ì†ë„ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'ë¹ ' if time['tempo_diff'] > 0 else 'ëŠë¦¼'}ë‹ˆë‹¤."
                )
            if abs(time["duration_ratio"] - 1.0) > 0.2:  # 20% ì´ìƒ ì°¨ì´
                feedback.append(
                    f"ì „ì²´ ë°œí™” ê¸¸ì´ê°€ ì›ì–´ë¯¼ë³´ë‹¤ {'ê¸¸' if time['duration_ratio'] > 1.0 else 'ì§§'}ìŠµë‹ˆë‹¤."
                )

            # ë¬¸ì¥ êµ¬ì¡° í”¼ë“œë°±
            if "structure" in prosody_result:
                structure = prosody_result["structure"]
                if structure["sentence_type"] == "question":
                    feedback.append(
                        "ì˜ë¬¸ë¬¸ì˜ ê²½ìš°, ë¬¸ì¥ ëì˜ ìŒë†’ì´ê°€ ì˜¬ë¼ê°€ì•¼ í•©ë‹ˆë‹¤."
                    )
                else:
                    feedback.append(
                        "í‰ì„œë¬¸ì˜ ê²½ìš°, ë¬¸ì¥ ëì˜ ìŒë†’ì´ê°€ ë‚´ë ¤ê°€ì•¼ í•©ë‹ˆë‹¤."
                    )

            # ì‹œê°í™” ì¶”ê°€
            if visualize:
                output_dir = os.path.join(self.config["output_dir"], "prosody_analysis")
                os.makedirs(output_dir, exist_ok=True)

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = os.path.join(
                    output_dir, f"prosody_comparison_{timestamp}.png"
                )

                try:
                    self.prosody_analyzer.visualize_prosody(
                        learner_audio,
                        native_audio,
                        output_path,
                        learner_textgrid=self.learner_textgrid,
                        native_textgrid=self.native_textgrid,
                    )
                    prosody_result["visualization_path"] = output_path
                except Exception as e:
                    logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

            return {"success": True, "feedback": feedback, "raw_data": prosody_result}

        except Exception as e:
            logger.error(f"ì–µì–‘/ê°•ì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def analyze_pronunciation(
        self,
        learner_audio: Optional[str] = None,
        native_audio: Optional[str] = None,
        script: Optional[str] = None,
    ) -> Dict:
        """ë°œìŒ ë¶„ì„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            learner_audio: í•™ìŠµì ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œ None)
            native_audio: ì›ì–´ë¯¼ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œ None)
            script: ëª©í‘œ ìŠ¤í¬ë¦½íŠ¸ (ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ)

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ë° ìƒíƒœ
        """
        result = {
            "success": False,
            "feedback": None,
            "error": None,
            "learner_text": None,
            "native_text": None,
            "script_text": None,
        }

        try:
            # ì…ë ¥ íŒŒì¼ ì„¤ì •
            if learner_audio:
                self.learner_audio = learner_audio
            if native_audio:
                self.native_audio = native_audio

            # 1. ì˜¤ë””ì˜¤ ë³€í™˜
            if not self.convert_audio(self.learner_audio, self.learner_wav):
                result["error"] = "í•™ìŠµì ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨"
                return result

            if not self.convert_audio(self.native_audio, self.native_wav):
                result["error"] = "ì›ì–´ë¯¼ ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨"
                return result
            # 2. Whisper ì „ì‚¬
            learner_text = self.transcribe_audio(
                self.learner_wav, self.learner_transcript
            )
            if not learner_text:
                result["error"] = "í•™ìŠµì ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤íŒ¨"
                return result

            native_text = self.transcribe_audio(self.native_wav, self.native_transcript)
            if not native_text:
                result["error"] = "ì›ì–´ë¯¼ ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤íŒ¨"
                return result

            # ê²°ê³¼ì— ì „ì‚¬ í…ìŠ¤íŠ¸ ì €ì¥
            result["learner_text"] = learner_text
            result["native_text"] = native_text

            # 3. ìŠ¤í¬ë¦½íŠ¸ ë¡œë”© ë˜ëŠ” ì„¤ì •
            if script:
                script_text = script
                # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì—ë„ ì €ì¥
                with open(self.script_path, "w", encoding="utf-8") as f:
                    f.write(script_text)
            else:
                try:
                    with open(self.script_path, "r", encoding="utf-8") as f:
                        script_text = f.read().strip()
                except FileNotFoundError:
                    # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ì–´ë¯¼ ì „ì‚¬ë¥¼ ì‚¬ìš©
                    script_text = native_text
                    with open(self.script_path, "w", encoding="utf-8") as f:
                        f.write(script_text)

            result["script_text"] = script_text

            # 4. MFA ì •ë ¬
            if not self.run_mfa_alignment(
                self.learner_wav, self.learner_transcript, "learner"
            ):
                result["error"] = "í•™ìŠµì ì˜¤ë””ì˜¤ ì •ë ¬ ì‹¤íŒ¨"
                return result

            if not self.run_mfa_alignment(
                self.native_wav, self.native_transcript, "native"
            ):
                result["error"] = "ì›ì–´ë¯¼ ì˜¤ë””ì˜¤ ì •ë ¬ ì‹¤íŒ¨"
                return result

            # 5. TextGrid ìš”ì•½
            learner_timing = self.summarize_textgrid(self.learner_textgrid)
            if not learner_timing:
                result["error"] = "í•™ìŠµì TextGrid ìš”ì•½ ì‹¤íŒ¨"
                return result

            native_timing = self.summarize_textgrid(self.native_textgrid)
            if not native_timing:
                result["error"] = "ì›ì–´ë¯¼ TextGrid ìš”ì•½ ì‹¤íŒ¨"
                return result

            # 6. ì–µì–‘/ê°•ì„¸ ë¶„ì„
            prosody_result = self.analyze_prosody(
                self.learner_wav,
                self.native_wav,
                learner_text,
                learner_timing,
                visualize=True,
            )

            # ê²°ê³¼ì— raw_data ì¶”ê°€
            if prosody_result and "raw_data" in prosody_result:
                result["raw_data"] = prosody_result["raw_data"]
                # ì‹œê°í™” ê²½ë¡œë„ ê²°ê³¼ì— ì¶”ê°€
                if "visualization_path" in prosody_result["raw_data"]:
                    result["visualization_path"] = prosody_result["raw_data"][
                        "visualization_path"
                    ]

            # 7. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.generate_prompt(
                learner_text,
                native_text,
                script_text,
                learner_timing,
                native_timing,
                prosody_result.get("feedback"),
            )

            # 8. GPT í”¼ë“œë°± ìƒì„±
            feedback = self.get_feedback(prompt)
            if not feedback:
                result["error"] = "í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨"
                return result

            # ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            result["success"] = True
            result["feedback"] = feedback

            return result

        except Exception as e:
            logger.error(f"ë°œìŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            result["error"] = str(e)
            return result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ì½”ì¹˜ ì´ˆê¸°í™”
        koach = Koach()

        # ë°œìŒ ë¶„ì„ ì‹¤í–‰
        result = koach.analyze_pronunciation()

        if result["success"]:
            # 1. GPT í”¼ë“œë°± ì¶œë ¥
            print("\nğŸ“£ ë°œìŒ í”¼ë“œë°± ê²°ê³¼:\n")
            print(result["feedback"])
            print("\n" + "=" * 50 + "\n")

            # 2. ì–µì–‘/ê°•ì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
            if "raw_data" in result and "visualization_path" in result["raw_data"]:
                print("ğŸµ ì–µì–‘/ê°•ì„¸ ë¶„ì„ ê²°ê³¼:")

                # í”¼ì¹˜(ìŒë†’ì´) ë¶„ì„ ê²°ê³¼
                pitch_data = result["raw_data"]["pitch"]
                print("\nìŒë†’ì´(í”¼ì¹˜) ë¶„ì„:")
                print(f"- í‰ê·  ì°¨ì´: {pitch_data['mean_diff']:.2f}")
                print(f"- í‘œì¤€í¸ì°¨ ì°¨ì´: {pitch_data['std_diff']:.2f}")
                print(f"- ë²”ìœ„ ì°¨ì´: {pitch_data['range_diff']:.2f}")

                # ì—ë„ˆì§€(ê°•ì„¸) ë¶„ì„ ê²°ê³¼
                energy_data = result["raw_data"]["energy"]
                print("\nê°•ì„¸(ì—ë„ˆì§€) ë¶„ì„:")
                print(f"- í‰ê·  ì°¨ì´: {energy_data['mean_diff']:.2f}")
                print(f"- í‘œì¤€í¸ì°¨ ì°¨ì´: {energy_data['std_diff']:.2f}")
                print(f"- í”¼í¬ ìˆ˜ ì°¨ì´: {energy_data['peaks_diff']}")

                # ì‹œê°„ ì˜ì—­ ë¶„ì„ ê²°ê³¼
                time_data = result["raw_data"]["time"]
                print("\nì‹œê°„ ì˜ì—­ ë¶„ì„:")
                print(f"- ë°œí™” ì†ë„ ì°¨ì´: {float(time_data['tempo_diff']):.2f} BPM")
                print(f"- ë°œí™” ê¸¸ì´ ë¹„ìœ¨: {float(time_data['duration_ratio']):.2f}")
                print(
                    f"- ìŒì ˆ ì‹œì‘ ê°•ë„ ì°¨ì´: {float(time_data['onset_strength_diff']):.2f}"
                )

                # 3. ì‹œê°í™” ê²°ê³¼ ì¶œë ¥
                print("\nğŸ“Š ì‹œê°í™” ê²°ê³¼:")
                print(f"ê²½ë¡œ: {result['raw_data']['visualization_path']}")

                # ì‹œê°í™” ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ì—´ê¸°
                system = platform.system()
                if system == "Darwin":  # macOS
                    subprocess.run(["open", result["raw_data"]["visualization_path"]])
                elif system == "Windows":
                    os.startfile(result["raw_data"]["visualization_path"])
                elif system == "Linux":
                    subprocess.run(
                        ["xdg-open", result["raw_data"]["visualization_path"]]
                    )

                print("\nì‹œê°í™” ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤.")
                print("ë¹¨ê°„ìƒ‰ ì„ : í•™ìŠµì ë°œí™”")
                print("íŒŒë€ìƒ‰ ì„ : ì›ì–´ë¯¼ ë°œí™”")
        else:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {result['error']}")

    except Exception as e:
        print(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
