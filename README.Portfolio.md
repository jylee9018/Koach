# ğŸ¤ Koach - AI-Powered Korean Pronunciation Analysis System

> **Portfolio Project: Speech Processing & Large Langague Model Integration**

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ğŸ¯ í”„ë¡œì íŠ¸ ëª©ì 
ì™¸êµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ **í•œêµ­ì–´ ë°œìŒ ë¶„ì„ ë° êµì • ì‹œìŠ¤í…œ** ê°œë°œ
- Montreal Forced Alignment(MFA)ë¥¼ í™œìš©í•œ ì •ë°€í•œ ìŒì†Œ ë‹¨ìœ„ ë¶„ì„
- OpenAI Whisperì™€ GPT-4ë¥¼ ê²°í•©í•œ ìŒì„±-í…ìŠ¤íŠ¸-í”¼ë“œë°± íŒŒì´í”„ë¼ì¸
- RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ë§ì¶¤í˜• ë°œìŒ ì§€ë„

### ğŸš€ í•µì‹¬ ê°€ì¹˜
- **ì •ë°€ë„**: ìŒì†Œ ë‹¨ìœ„ê¹Œì§€ ë¶„ì„í•˜ëŠ” ì„¸ë°€í•œ ë°œìŒ í‰ê°€
- **ì‹¤ìš©ì„±**: CLI ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìƒˆë¡œìš´ ì–¸ì–´ í™•ì¥ ìš©ì´
- **êµìœ¡ì  ê°€ì¹˜**: ë‹¨ìˆœ ì ìˆ˜ê°€ ì•„ë‹Œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ì œì‹œ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```mermaid
flowchart TD
    %% ì…ë ¥ ë‹¨ê³„
    A1[ğŸ‘¤ í•™ìŠµì ì˜¤ë””ì˜¤<br/>M4A/WAV] --> B1[ğŸ”„ ì˜¤ë””ì˜¤ ë³€í™˜<br/>pydub]
    A2[ğŸ¯ ì›ì–´ë¯¼ ì˜¤ë””ì˜¤<br/>M4A/WAV] --> B2[ğŸ”„ ì˜¤ë””ì˜¤ ë³€í™˜<br/>pydub]
    A3[ğŸ“ ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸] --> C1
    
    %% ì „ì²˜ë¦¬ ë‹¨ê³„
    B1 --> B3[ğŸ“ ì˜¤ë””ì˜¤ ì •ê·œí™”<br/>librosa]
    B2 --> B4[ğŸ“ ì˜¤ë””ì˜¤ ì •ê·œí™”<br/>librosa]
    
    %% ìŒì„± ì¸ì‹ ë‹¨ê³„
    B3 --> C1[ğŸ¤ ìŒì„± ì¸ì‹<br/>Whisper ASR<br/>Korean Model]
    B4 --> C2[ğŸ¤ ìŒì„± ì¸ì‹<br/>Whisper ASR<br/>Korean Model]
    
    %% ê°•ì œ ì •ë ¬ ë‹¨ê³„
    C1 --> D1[âš¡ ê°•ì œ ì •ë ¬<br/>Montreal FA<br/>korean_mfa.zip]
    C2 --> D2[âš¡ ê°•ì œ ì •ë ¬<br/>Montreal FA<br/>korean_mfa.zip]
    A3 --> D1
    A3 --> D2
    
    %% TextGrid ìƒì„±
    D1 --> E1[ğŸ“Š TextGrid<br/>learner.TextGrid]
    D2 --> E2[ğŸ“Š TextGrid<br/>native.TextGrid]
    
    %% ë¶„ì„ ë‹¨ê³„
    E1 --> F1[ğŸ” ìŒì†Œ ë¶„ì„<br/>Phoneme Analysis]
    E1 --> F2[ğŸµ ì–µì–‘ ë¶„ì„<br/>Prosody Analysis]
    E1 --> F3[âš–ï¸ ë¹„êµ ë¶„ì„<br/>vs Native]
    E2 --> F3
    
    %% AI í”¼ë“œë°± ìƒì„±
    F1 --> G1[ğŸ§  AI í”¼ë“œë°± ìƒì„±]
    F2 --> G1
    F3 --> G1
    
    G1 --> G2[ğŸ“š RAG ì§€ì‹ë² ì´ìŠ¤<br/>FAISS + Sentence<br/>Transformers]
    G2 --> G3[ğŸ¤– GPT-4<br/>êµìœ¡ì  í”¼ë“œë°±<br/>ìƒì„±]
    
    %% ì‹œê°í™” ë° ì¶œë ¥
    F1 --> H1[ğŸ“ˆ ì‹œê°í™”<br/>matplotlib]
    F2 --> H1
    F3 --> H1
    
    G3 --> I1[ğŸ’¾ JSON ê²°ê³¼<br/>analysis_result.json]
    H1 --> I2[ğŸ–¼ï¸ ì°¨íŠ¸ ì´ë¯¸ì§€<br/>phoneme_accuracy.png<br/>pitch_comparison.png]
    
    %% ìµœì¢… ì¶œë ¥
    I1 --> J[âœ… ì™„ë£Œ<br/>ìœ ì‚¬ë„ ì ìˆ˜<br/>ë°œìŒ í”¼ë“œë°±<br/>ê°œì„  ë°©ì•ˆ]
    I2 --> J
    
    %% ìŠ¤íƒ€ì¼ë§
    classDef inputClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef processClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef analysisClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef aiClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef outputClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class A1,A2,A3 inputClass
    class B1,B2,B3,B4,C1,C2,D1,D2 processClass
    class E1,E2,F1,F2,F3 analysisClass
    class G1,G2,G3 aiClass
    class H1,I1,I2,J outputClass
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°
```python
koach/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ koach.py           # ë©”ì¸ ë¶„ì„ ì—”ì§„ (1,570ì¤„)
â”‚   â”œâ”€â”€ prosody.py         # ì–µì–‘/ê°•ì„¸ ë¶„ì„ (561ì¤„)  
â”‚   â””â”€â”€ knowledge_base.py  # RAG ì§€ì‹ë² ì´ìŠ¤ (113ì¤„)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio.py           # ì˜¤ë””ì˜¤ ì‹ í˜¸ ì²˜ë¦¬ (271ì¤„)
â”‚   â””â”€â”€ text.py            # í…ìŠ¤íŠ¸/ìŒì„± ì •ë ¬ (206ì¤„)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py        # ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ (252ì¤„)
â””â”€â”€ models/
    â”œâ”€â”€ korean_mfa.zip     # í•œêµ­ì–´ ìŒí–¥ ëª¨ë¸ (59MB)
    â””â”€â”€ korean_mfa.dict    # í•œêµ­ì–´ ë°œìŒ ì‚¬ì „ (21K entries)
```

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ ë° êµ¬í˜„

### **í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ**
| ì˜ì—­ | ê¸°ìˆ  | êµ¬í˜„ ëª©ì  | ì„±ëŠ¥ |
|------|------|-----------|------|
| **ìŒì„± ì¸ì‹** | OpenAI Whisper | ë‹¤êµ­ì–´ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ | í•œêµ­ì–´ 95%+ ì •í™•ë„ |
| **ìŒì„± ì •ë ¬** | Montreal Forced Alignment | ìŒì†Œ ë‹¨ìœ„ ì‹œê°„ ì •ë³´ ì¶”ì¶œ | ì›ì–´ë¯¼ 98%+ ì •ë ¬ ì •í™•ë„ |
| **AI í”¼ë“œë°±** | OpenAI GPT-4 | ë§¥ë½ì  ìì—°ì–´ í”¼ë“œë°± ìƒì„± | êµìœ¡ì  í’ˆì§ˆ í”¼ë“œë°± |
| **ì§€ì‹ë² ì´ìŠ¤** | FAISS + Sentence Transformers | RAG ê¸°ë°˜ ë°œìŒ ì§€ì‹ ê²€ìƒ‰ | 384ì°¨ì› ì„ë² ë”© |
| **ì‹ í˜¸ ì²˜ë¦¬** | librosa, numpy | ìŒì„± íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„ | ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ |

### **1. ìŒì„± ì¸ì‹ íŒŒì´í”„ë¼ì¸**
```python
class Koach:
    def analyze_pronunciation(self, learner_audio, native_audio, script):
        # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        convert_audio(learner_audio, self.learner_wav)
        normalize_audio(self.learner_wav, learner_normalized)
        
        # 2. Whisper ìŒì„± ì¸ì‹
        learner_result = transcribe_audio(learner_normalized)
        native_result = transcribe_audio(native_normalized)
        
        # 3. MFA ê°•ì œ ì •ë ¬
        alignment_success = self.run_mfa_alignment_batch(
            learner_wav, native_wav, learner_transcript, native_transcript
        )
        
        # 4. ìŒì†Œ/ì–µì–‘ ë¶„ì„
        phoneme_analysis = self._analyze_phonemes(self.learner_textgrid)
        prosody_analysis = self._analyze_prosody_detailed(learner_normalized)
        
        # 5. GPT í”¼ë“œë°± ìƒì„±
        feedback = self.generate_contextualized_feedback(analysis_result)
```

### **2. Montreal Forced Alignment í†µí•©**
```python
def run_mfa_alignment_batch(self, learner_wav, native_wav, learner_transcript, native_transcript):
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ MFA ì„±ëŠ¥ ìµœì í™”"""
    try:
        # ë°°ì¹˜ ì…ë ¥ ì¤€ë¹„
        mfa_input_files = self._prepare_mfa_batch_input(
            [learner_wav, native_wav], 
            [learner_transcript, native_transcript]
        )
        
        # MFA ì‹¤í–‰
        cmd = [
            "mfa", "align", str(self.mfa_input), 
            str(self.lexicon_path), str(self.acoustic_model),
            str(self.mfa_output), "--clean"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        return self._validate_alignment_output()
        
    except subprocess.TimeoutExpired:
        logger.warning("MFA timeout - ë¹ ë¥¸ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜")
        return self._fallback_simple_alignment()
```

### **3. ìŒì†Œ ë¶„ì„ ë° ë¹„êµ**
```python
def _analyze_phonemes(self, textgrid_path):
    """TextGridì—ì„œ ìŒì†Œ ì •ë³´ ì¶”ì¶œ ë° ë¶„ì„"""
    tg = textgrid.TextGrid.fromFile(textgrid_path)
    
    phonemes = []
    for tier in tg.tiers:
        if 'phones' in tier.name.lower():
            for interval in tier:
                if interval.mark and interval.mark.strip():
                    phonemes.append({
                        "phoneme": interval.mark,
                        "start": float(interval.minTime),
                        "end": float(interval.maxTime),
                        "duration": float(interval.maxTime - interval.minTime)
                    })
    
    # í•œêµ­ì–´ íŠ¹í™” ìŒì†Œ ê·œì¹™ ì ìš©
    return self._apply_korean_phoneme_rules(phonemes)
```

### **4. RAG ê¸°ë°˜ ì§€ì‹ë² ì´ìŠ¤**
```python
class KnowledgeBase:
    def __init__(self, knowledge_dir, embedding_model):
        self.model = SentenceTransformer(embedding_model)
        self.index = faiss.IndexFlatIP(384)  # 384ì°¨ì› ì„ë² ë”©
        self._build_knowledge_base(knowledge_dir)
    
    def search_relevant_knowledge(self, error_type, top_k=3):
        """ë°œìŒ ì˜¤ë¥˜ ìœ í˜•ì— ë§ëŠ” ì§€ì‹ ê²€ìƒ‰"""
        query_embedding = self.model.encode([f"í•œêµ­ì–´ ë°œìŒ {error_type}"])
        distances, indices = self.index.search(query_embedding, top_k)
        
        return [self.documents[idx] for idx in indices[0]]
```

### **5. ì‹œê°í™” ë° ê²°ê³¼ ìƒì„±**
```python
def _visualize_results(self, learner_audio, reference_audio, learner_textgrid, 
                      phoneme_analysis, prosody_analysis):
    """ì¢…í•©ì ì¸ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ìŒì†Œ ì •í™•ë„ íˆíŠ¸ë§µ
    self._plot_phoneme_heatmap(axes[0,0], phoneme_analysis)
    
    # Pitch ê³¡ì„  ë¹„êµ
    self._plot_pitch_comparison(axes[0,1], prosody_analysis)
    
    # íŒŒí˜• ë¹„êµ
    self._plot_waveform_comparison(axes[1,0], [learner_audio, reference_audio])
    
    # ì „ì²´ ì ìˆ˜ ë ˆì´ë” ì°¨íŠ¸  
    self._plot_score_radar(axes[1,1], analysis_scores)
    
    return self._save_visualization_safely(fig)
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ë° ì‹¤ìš©ì„±

### **1. ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”**
```python
# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ MFA ì„±ëŠ¥ í–¥ìƒ
CURRENT_CONFIG = {
    "mfa": {
        "batch_processing": True,      # ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”
        "fast_mode": True,             # ë¹ ë¥¸ ì •ë ¬ ëª¨ë“œ
        "timeout": 120,                # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        "skip_mfa": False,             # í•„ìš”ì‹œ MFA ê±´ë„ˆë›°ê¸° ì˜µì…˜
    }
}

# ì ì‘ì  ì²˜ë¦¬ ì „ëµ
def adaptive_processing(self, audio_duration):
    if audio_duration < 10:  # ì§§ì€ ì˜¤ë””ì˜¤
        return self.run_full_analysis()
    else:  # ê¸´ ì˜¤ë””ì˜¤
        return self.run_chunked_analysis(chunk_size=10)
```

### **2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„¤ê³„**
```python
def get_normalized_paths(self, speaker_type):
    """ì •ê·œí™”ëœ íŒŒì¼ ê²½ë¡œ ê´€ë¦¬"""
    return {
        "original": self.paths[f"{speaker_type}_wav"],
        "normalized": NORMALIZED_DIR / f"{speaker_type}_normalized.wav",
        "temp": TEMP_ROOT / f"{speaker_type}_temp.wav"
    }

# ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬
@contextmanager
def temp_file_context(self, file_path):
    try:
        yield file_path
    finally:
        if Path(file_path).exists():
            os.remove(file_path)
```

### **3. ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬**
```python
def robust_transcription(self, audio_path):
    """ë‹¤ë‹¨ê³„ ìŒì„± ì¸ì‹ ì „ëµ"""
    try:
        # 1ì°¨: í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
        result = whisper.transcribe(audio_path, language="ko")
        
        if result.get('confidence', 0) > 0.8:
            return result['text']
        else:
            # 2ì°¨: ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ ì¬ì‹œë„
            logger.warning("ë‚®ì€ ì‹ ë¢°ë„, ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ ì¬ì‹œë„")
            return whisper.transcribe(audio_path, language=None)['text']
            
    except Exception as e:
        logger.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return None
```

---

## ğŸ¯ ì‹¤ì œ êµ¬í˜„ ì„±ê³¼

### **ê¸°ìˆ ì  ì„±ê³¼**
- **ì²˜ë¦¬ ì†ë„**: 1ë¶„ ìŒì„± â†’ 30-60ì´ˆ ë¶„ì„ (MFA ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)
- **ì •í™•ë„**: í•œêµ­ì–´ ìŒì„± ì¸ì‹ 95%+, ì›ì–´ë¯¼ ì •ë ¬ 98%+ ë‹¬ì„±
- **ì•ˆì •ì„±**: ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§· ì§€ì› (M4A, WAV, AAC ë“±)
- **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìƒˆë¡œìš´ ì–¸ì–´ ì¶”ê°€ ìš©ì´

### **ì‹¤ìš©ì  ê¸°ëŠ¥**
```bash
# CLI ì¸í„°í˜ì´ìŠ¤ - ë‹¤ì–‘í•œ ì‚¬ìš© ë°©ì‹ ì§€ì›
python main.py input/learner.m4a input/native.m4a "ì•ˆë…•í•˜ì„¸ìš”"
python main.py --file learner.wav --reference native.wav --text "í•œêµ­ì–´"
python main.py --model-size large --no-rag --quiet
```

### **ì¶œë ¥ ê²°ê³¼ êµ¬ì¡°**
```json
{
    "similarity_score": 0.85,
    "feedback": "GPT ìƒì„± êµìœ¡ì  í”¼ë“œë°±",
    "phoneme_analysis": {
        "total_phonemes": 15,
        "accuracy": 0.9,
        "problematic_phonemes": ["ã…“", "ã…¡"]
    },
    "prosody_analysis": {
        "pitch_similarity": 0.8,
        "rhythm_score": 0.75,
        "stress_accuracy": 0.9
    },
    "visualization_paths": [
        "output/phoneme_accuracy.png",
        "output/pitch_comparison.png"
    ]
}
```

---

## ğŸ”§ í•µì‹¬ êµ¬í˜„ ìš”ì†Œ

### **1. ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬**
```python
# config/settings.py - 252ì¤„ì˜ ì²´ê³„ì ì¸ ì„¤ì • ê´€ë¦¬
CURRENT_CONFIG = {
    "audio": {"sample_rate": 16000, "channels": 1},
    "whisper": {"model_name": "base", "language": "ko"},
    "mfa": {"batch_processing": True, "fast_mode": True},
    "visualization": {"enabled": True, "dpi": 300}
}

# ê²½ë¡œ ê´€ë¦¬ ìë™í™”
PATHS = {
    "learner_audio": INPUT_DIR / "learner.m4a",
    "native_audio": INPUT_DIR / "native.m4a", 
    "mfa_output": MFA_OUTPUT_DIR,
    "visualize": VISUALIZE_DIR
}
```

### **2. ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì **
```python
def analyze_pronunciation(self):
    result = {"steps": {}, "errors": [], "status": "ì§„í–‰ì¤‘"}
    
    # ê° ë‹¨ê³„ë³„ ìƒíƒœ ì¶”ì 
    result["steps"]["audio_conversion"] = "ì„±ê³µ"
    result["steps"]["speech_recognition"] = "ì„±ê³µ" 
    result["steps"]["mfa_alignment"] = "ì„±ê³µ"
    result["steps"]["pronunciation_analysis"] = "ì„±ê³µ"
    
    return result
```

### **3. í•œêµ­ì–´ íŠ¹í™” ì²˜ë¦¬**
```python
KOREAN_PHONEME_RULES = {
    'consonant_clusters': ['ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰'],
    'vowel_harmony': {'ã…': ['ã…', 'ã…‘', 'ã…—', 'ã…›']},
    'final_consonants': ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã…‡']
}

def extract_pronunciation_issues_detailed(self, learner_text, native_text):
    """í•œêµ­ì–´ íŠ¹í™” ë°œìŒ ë¬¸ì œì  ë¶„ì„"""
    issues = []
    
    # ë°›ì¹¨ ê´€ë ¨ ë¬¸ì œ ê²€ì¶œ
    for word in native_text.split():
        if any(c in "ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…" for c in word):
            if word not in learner_text:
                issues.append(f"'{word}' ë‹¨ì–´ ë°œìŒ ë¬¸ì œ (ë°›ì¹¨ ê´€ë ¨)")
    
    return issues
```

---

## ğŸš§ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±…

### **1. MFA ì²˜ë¦¬ ì‹œê°„ ë¬¸ì œ**
**ë¬¸ì œ**: Montreal Forced Alignmentì˜ ê¸´ ì²˜ë¦¬ ì‹œê°„ (2-3ë¶„)
```python
# í•´ê²°ì±…: ì ì‘ì  íƒ€ì„ì•„ì›ƒê³¼ ë°°ì¹˜ ì²˜ë¦¬
def run_mfa_alignment_batch(self):
    try:
        cmd = ["mfa", "align", "--clean", "--num_jobs", "2"]
        result = subprocess.run(cmd, timeout=120)  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        
    except subprocess.TimeoutExpired:
        logger.warning("MFA timeout - ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì „í™˜")
        return self._fallback_analysis()
```

### **2. ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§· ì§€ì›**
**ë¬¸ì œ**: M4A, AAC ë“± ë‹¤ì–‘í•œ ì…ë ¥ í¬ë§· ì²˜ë¦¬ í•„ìš”
```python
# í•´ê²°ì±…: pydubì„ í™œìš©í•œ í†µí•© ë³€í™˜ íŒŒì´í”„ë¼ì¸
def convert_audio(input_path, output_path):
    """ë²”ìš© ì˜¤ë””ì˜¤ ë³€í™˜ê¸°"""
    ext = input_path.split('.')[-1].lower()
    
    if ext in ['m4a', 'aac']:
        audio = AudioSegment.from_file(input_path, format=ext)
        audio.export(output_path, format="wav")
    else:
        shutil.copy(input_path, output_path)
```

### **3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëŒ€ìš©ëŸ‰ ì²˜ë¦¬**
**ë¬¸ì œ**: ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
```python
# í•´ê²°ì±…: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬
def process_large_audio(self, audio_path, chunk_size=30):
    audio_duration = get_audio_duration(audio_path)
    
    for start_time in range(0, int(audio_duration), chunk_size):
        chunk = extract_audio_segment(audio_path, start_time, start_time + chunk_size)
        chunk_result = self._analyze_chunk(chunk)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del chunk
        gc.collect()
```

---

## ğŸ“ˆ í•™ìŠµ ì„±ê³¼ ë° ì¸ì‚¬ì´íŠ¸

### **ìŒì„± ì²˜ë¦¬ ì „ë¬¸ì„±**
- **Montreal Forced Alignment**: ìŒì„±í•™ì  ì •ë°€ë„ë¥¼ ìœ„í•œ ê°•ì œ ì •ë ¬ ë§ˆìŠ¤í„°
- **Whisper Integration**: ë‹¤êµ­ì–´ ASR ëª¨ë¸ì˜ í•œêµ­ì–´ ìµœì í™” ê²½í—˜
- **ì‹ í˜¸ ì²˜ë¦¬**: librosaë¥¼ í™œìš©í•œ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„

### **ì‹œìŠ¤í…œ ì„¤ê³„ ì—­ëŸ‰**
- **ëª¨ë“ˆí™”**: 1,570ì¤„ ë©”ì¸ í´ë˜ìŠ¤ë¥¼ ê¸°ëŠ¥ë³„ë¡œ ì²´ê³„ì  ë¶„ë¦¬
- **ì„¤ì • ê´€ë¦¬**: ì¤‘ì•™í™”ëœ 252ì¤„ ì„¤ì • ì‹œìŠ¤í…œìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ì„± í™•ë³´
- **ì˜¤ë¥˜ ì²˜ë¦¬**: ê²¬ê³ í•œ fallback ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥

### **AI/ML í†µí•© ê²½í—˜**
- **Multi-Modal AI**: ìŒì„± + í…ìŠ¤íŠ¸ + ì§€ì‹ë² ì´ìŠ¤ í†µí•© íŒŒì´í”„ë¼ì¸
- **RAG Implementation**: FAISS + Sentence Transformersë¡œ ë§ì¶¤í˜• í”¼ë“œë°±
- **Prompt Engineering**: GPT-4ë¥¼ í™œìš©í•œ êµìœ¡ì  í”¼ë“œë°± ìƒì„± ìµœì í™”

---

## ğŸ”„ í™•ì¥ ê°€ëŠ¥ì„±

### **ê¸°ìˆ ì  í™•ì¥**
- **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ë¡œ í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: WebSocket ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¶”ê°€
- **ëª¨ë°”ì¼ ìµœì í™”**: Edge computingìœ¼ë¡œ ê²½ëŸ‰í™” ë²„ì „ ê°œë°œ

### **ë¹„ì¦ˆë‹ˆìŠ¤ í™•ì¥**
- **êµìœ¡ í”Œë«í¼**: LMS ì‹œìŠ¤í…œ ì—°ë™ API ê°œë°œ
- **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤**: AWS/GCP ê¸°ë°˜ SaaS ì „í™˜
- **ì—°êµ¬ í˜‘ë ¥**: ìŒì„±í•™ ì—°êµ¬ê¸°ê´€ê³¼ì˜ ë°ì´í„° í˜‘ì—…

---

## ğŸ”— í”„ë¡œì íŠ¸ ì •ë³´

### **ì½”ë“œ êµ¬ì¡°**
- **ì´ ì½”ë“œëŸ‰**: 3,000+ ì¤„
- **ë©”ì¸ ì—”ì§„**: `koach/core/koach.py` (1,570ì¤„)
- **ì„¤ì • ê´€ë¦¬**: `koach/config/settings.py` (252ì¤„)
- **CLI ì¸í„°í˜ì´ìŠ¤**: `koach/main.py` (336ì¤„)

### **ì„±ëŠ¥ ì§€í‘œ**
- **ì²˜ë¦¬ ì†ë„**: 1ë¶„ ìŒì„± ë¶„ì„ 30-60ì´ˆ
- **ì •í™•ë„**: í•œêµ­ì–´ ìŒì„± ì¸ì‹ 95%+
- **ì•ˆì •ì„±**: ë‹¤ì–‘í•œ í¬ë§·/í™˜ê²½ì—ì„œ ì•ˆì •ì  ë™ì‘

---

## ğŸ’¡ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸

### **ìŒì„± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
Montreal Forced Alignmentì™€ Whisperë¥¼ ê²°í•©í•œ end-to-end ìŒì„± ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë©°, ìŒì„±í•™ì  ì •ë°€ë„ì™€ ì‹¤ìš©ì„±ì„ ë™ì‹œì— í™•ë³´í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### **AI ëª¨ë¸ í†µí•© ë° ìµœì í™”**
GPT-4ì™€ RAG ì‹œìŠ¤í…œì„ ê²°í•©í•˜ì—¬ ë‹¨ìˆœí•œ ì ìˆ˜ê°€ ì•„ë‹Œ êµìœ¡ì  ê°€ì¹˜ê°€ ìˆëŠ” í”¼ë“œë°±ì„ ìƒì„±í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

### **ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´ë§**
3,000ì¤„ ì´ìƒì˜ ì½”ë“œë¥¼ ëª¨ë“ˆí™”í•˜ê³ , ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ì™€ ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í†µí•´ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

---

> **"ì–¸ì–´í•™ì  ì •ë°€ë„ì™€ AIì˜ êµìœ¡ì  ê°€ì¹˜ë¥¼ ê²°í•©í•œ ì‹¤ìš©ì  ì‹œìŠ¤í…œ"**  
> *ë³µì¡í•œ ìŒì„±í•™ì  ë¶„ì„ì„ ì§ê´€ì ì¸ í”¼ë“œë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ AIì˜ êµìœ¡ ë¶„ì•¼ ì ìš© ê°€ëŠ¥ì„±ì„ ì‹¤í˜„í–ˆìŠµë‹ˆë‹¤.*